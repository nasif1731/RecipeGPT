{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5093016,"sourceType":"datasetVersion","datasetId":2957522}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport ast\nfrom pathlib import Path\n\n\nDATA_PATH = Path(\"/kaggle/input/3a2mext/3A2M_EXTENDED.csv\")\nMAX_SAMPLES = 100_000\n\n\nprint(\"Loading dataset...\")\ndf = pd.read_csv(DATA_PATH)\nif MAX_SAMPLES:\n    df = df.head(MAX_SAMPLES)\nprint(f\"Loaded {len(df)} samples.\")\n\n\ndef safe_parse_list(val):\n    if isinstance(val, str) and val.strip().startswith(\"[\"):\n        try:\n            parsed = ast.literal_eval(val)\n            if isinstance(parsed, list):\n                return parsed\n        except (ValueError, SyntaxError):\n            return []\n    return []\n\nprint(\"Parsing NER and directions columns...\")\ndf['NER'] = df['NER'].apply(safe_parse_list)\ndf['directions'] = df['directions'].apply(safe_parse_list)\n\ndef format_example(title, ingredients, directions):\n    ingredients_str = \", \".join(ingredients)\n    directions_str = \" \".join(directions)\n    return f\"Title: {title}\\nIngredients: {ingredients_str}\\nDirections: {directions_str}\"\n\ndf['formatted_text'] = df.apply(\n    lambda row: format_example(row['title'], row['NER'], row['directions']), axis=1\n)\n\n\ndf = df[df['formatted_text'].str.len() > 50].reset_index(drop=True)\n\n\nprint(\"\\nSample formatted entry:\\n\")\nprint(df['formatted_text'].iloc[0][:1000])  # show only the first 1000 chars\nprint(\"\\nTotal valid samples:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T06:16:35.105455Z","iopub.execute_input":"2025-11-02T06:16:35.106290Z","iopub.status.idle":"2025-11-02T06:17:19.991415Z","shell.execute_reply.started":"2025-11-02T06:16:35.106246Z","shell.execute_reply":"2025-11-02T06:17:19.990600Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nLoaded 100000 samples.\nParsing NER and directions columns...\n\nSample formatted entry:\n\nTitle: \t Arugula Pomegranate Salad\nIngredients: baby spinach, baby arugula, pomegranate arils, persimmon, alfalfa sprouts\nDirections: Toss together spinach and arugula, then place in your serving bowl. Remove the stem and leaves of the persimmon, then slice into thin wedges. Arrange the persimmon on top of the spinach and arugula. Garnish with pomegranate arils and alfalfa sprouts.\n\nTotal valid samples: 99997\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nimport ast \nimport os\nimport time\n\n\nDATASET_PATH = \"/kaggle/input/3a2mext/3A2M_EXTENDED.csv\"\n\n\nOUTPUT_DIR = \"/kaggle/working/\"\nOUTPUT_FILE_TRAIN = os.path.join(OUTPUT_DIR, \"train_recipes.txt\")\nOUTPUT_FILE_VAL = os.path.join(OUTPUT_DIR, \"val_recipes.txt\")\n\n\nBOS_TOKEN = \"<|startofrecipe|>\"\nGENRE_TOKEN = \"<|genre|>\"\nTITLE_TOKEN = \"<|title|>\"\nINGREDIENTS_TOKEN = \"<|ingredients|>\"\nSTEPS_TOKEN = \"<|steps|>\"\nEOS_TOKEN = \"<|endofrecipe|>\"\n\n\ndef clean_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndef safe_eval_list(raw_list_str):\n\n    if not isinstance(raw_list_str, str) or not raw_list_str.startswith('['):\n        return []\n    try:\n        return ast.literal_eval(raw_list_str)\n    except (ValueError, SyntaxError):\n        return []\n\ndef format_recipe(row):\n    try:\n        genre = clean_text(row['genre'])\n        title = clean_text(row['title'])\n        ner_list = safe_eval_list(row['NER'])\n        ext_ner_list = safe_eval_list(row['Extended_NER'])\n        combined_entities = sorted(list(set(ner_list + ext_ner_list)), key=str.lower)\n        ingredients_str = ', '.join(combined_entities)\n        directions_list = safe_eval_list(row['directions'])\n        steps = ' '.join(f\"{i+1}. {clean_text(step)}\" for i, step in enumerate(directions_list))\n        if not genre or not title or not ingredients_str or not steps:\n            return None\n            \n        formatted_string = (\n            f\"{BOS_TOKEN}\"\n            f\"{GENRE_TOKEN}{genre}\"\n            f\"{TITLE_TOKEN}{title}\"\n            f\"{INGREDIENTS_TOKEN}{ingredients_str}\"\n            f\"{STEPS_TOKEN}{steps}\"\n            f\"{EOS_TOKEN}\\n\" # Add newline to separate recipes\n        )\n        return formatted_string\n        \n    except Exception as e:\n        return None\n\ndef run_data_prep():\n    print(f\"Loading dataset from {DATASET_PATH}...\")\n    start_time = time.time()\n    try:\n        columns_to_load = ['title', 'NER', 'Extended_NER', 'genre', 'directions']\n        df = pd.read_csv(DATASET_PATH, usecols=columns_to_load)\n        \n    except FileNotFoundError:\n        print(f\"Error: Dataset file not found at {DATAFSET_PATH}\")\n        print(\"Please ensure your that the '3a2mext' dataset added as input.\")\n        return\n    except ValueError as e:\n        print(f\"Error loading CSV.{e}\")\n        return\n\n    df = df.dropna()\n    load_time = time.time()\n    print(f\"Loaded {len(df)} non-null recipes in {load_time - start_time:.2f} seconds.\")\n\n    formatted_recipes = df.apply(format_recipe, axis=1).tolist()\n    formatted_recipes = [r for r in formatted_recipes if r is not None]\n    \n    format_time = time.time()\n    print(f\"Successfully formatted {len(formatted_recipes)} recipes in {format_time - load_time:.2f} seconds.\")\n\n    if not formatted_recipes:\n        print(\"No recipes were formatted. Please check dataset columns and helper functions.\")\n        return\n\n    \n    train_data, val_data = train_test_split(formatted_recipes, test_size=0.1, random_state=30)\n\n    with open(OUTPUT_FILE_TRAIN, \"w\", encoding=\"utf-8\") as f:\n        f.writelines(train_data)\n    \n    with open(OUTPUT_FILE_VAL, \"w\", encoding=\"utf-8\") as f:\n        f.writelines(val_data)\n\n    write_time = time.time()\n    print(f\"Successfully saved {len(train_data)} training recipes to {OUTPUT_FILE_TRAIN}\")\n    print(f\"Successfully saved {len(val_data)} validation recipes to {OUTPUT_FILE_VAL}\")\n    print(f\"Files written in {write_time - format_time:.2f} seconds.\")\n    print(\"\\n Example of new formatted recipe \")\n    print(train_data[0][:500] + \"...\")\n    \n    total_time = time.time() - start_time\n    print(f\"\\nData Formatting is complete. Total time: {total_time:.2f} seconds.\")\n\n\nrun_data_prep()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T06:26:56.481925Z","iopub.execute_input":"2025-11-02T06:26:56.482286Z","iopub.status.idle":"2025-11-02T06:32:09.982072Z","shell.execute_reply.started":"2025-11-02T06:26:56.482258Z","shell.execute_reply":"2025-11-02T06:32:09.981234Z"}},"outputs":[{"name":"stdout","text":"Loading dataset from /kaggle/input/3a2mext/3A2M_EXTENDED.csv...\nLoaded 2231142 non-null recipes in 22.34 seconds.\nSuccessfully formatted 1964305 recipes in 284.74 seconds.\nSuccessfully saved 1767874 training recipes to /kaggle/working/train_recipes.txt\nSuccessfully saved 196431 validation recipes to /kaggle/working/val_recipes.txt\nFiles written in 5.32 seconds.\n\n Example of new formatted recipe \n<|startofrecipe|><|genre|>bakery<|title|>Christmas Fudge<|ingredients|>1 minute, 4 to 5 minutes, 8-inch, Bring, butter, Combine, Cook, Cool, marshmallows, milk, salt, semi-sweet chocolate chips, semisweet chocolate chips, Stir, sugar, vanilla<|steps|>1. Cream sugar, syrup and salt; cook in large pan. 2. Cook until sugar dissolves (boil 1 minute covered). 3. Uncover; cook to soft ball stage. 4. Add vanilla; beat on high speed mixer until creamy. Add all other ingredients. 5. Pour into square pan....\n\nData Formatting is complete. Total time: 312.39 seconds.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\n!pip install -q \"transformers==4.38.2\" \"datasets==2.18.0\" \"accelerate==0.27.2\" \"huggingface_hub==0.20.3\" \"peft==0.9.0\"\n\nprint(\"Install complete.\")\n\nimport os\nos._exit(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T06:34:34.163371Z","iopub.execute_input":"2025-11-02T06:34:34.163851Z","execution_failed":"2025-11-02T06:34:38.503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import (\n    GPT2Tokenizer, \n    GPT2LMHeadModel, \n    Trainer, \n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import load_dataset\nprint(\"Libraries imported successfully.\")\n\n\nINPUT_DIR = \"/kaggle/working/\"\nTRAIN_FILE = os.path.join(INPUT_DIR, \"train_recipes.txt\")\nVAL_FILE = os.path.join(INPUT_DIR, \"val_recipes.txt\")\nOUTPUT_DIR = os.path.join(INPUT_DIR, \"recipe-gpt2-model\")\nMODEL_NAME = \"openai-community/gpt2\" # Use the full, correct name\n\n\nBOS_TOKEN = \"<|startofrecipe|>\"\nGENRE_TOKEN = \"<|genre|>\"\nTITLE_TOKEN = \"<|title|>\"\nINGREDIENTS_TOKEN = \"<|ingredients|>\"\nSTEPS_TOKEN = \"<|steps|>\"\nEOS_TOKEN = \"<|endofrecipe|>\"\nPAD_TOKEN = \"<|pad|>\"\n\nSPECIAL_TOKENS = {\n    \"bos_token\": BOS_TOKEN,\n    \"eos_token\": EOS_TOKEN,\n    \"pad_token\": PAD_TOKEN,\n    \"additional_special_tokens\": [GENRE_TOKEN, TITLE_TOKEN, INGREDIENTS_TOKEN, STEPS_TOKEN]\n}\n\nprint(\"Loading tokenizer...\")\ntokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, use_fast=False)\nprint(\"Tokenizer loaded successfully.\")\n\nprint(f\"Original tokenizer vocab size: {len(tokenizer)}\")\n\n\nnum_added_tokens = tokenizer.add_special_tokens(SPECIAL_TOKENS)\n\ntokenizer.pad_token = PAD_TOKEN\n\nprint(f\"Added {num_added_tokens} new tokens.\")\nprint(f\"New tokenizer vocab size: {len(tokenizer)}\")\nprint(f\"Tokenizer pad token set to: {tokenizer.pad_token}\")\n\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Tokenizer saved to {OUTPUT_DIR}\")\nprint(\"\\n--- SETUP COMPLETE ---\")\nprint(\"All libraries and the tokenizer are now correctly loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T06:35:25.245466Z","iopub.execute_input":"2025-11-02T06:35:25.245948Z","iopub.status.idle":"2025-11-02T06:35:33.299519Z","shell.execute_reply.started":"2025-11-02T06:35:25.245925Z","shell.execute_reply":"2025-11-02T06:35:33.298643Z"}},"outputs":[{"name":"stderr","text":"2025-11-02 06:35:29.878941: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762065329.903494    3458 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762065329.910678    3458 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Libraries imported successfully.\nLoading tokenizer...\nTokenizer loaded successfully.\nOriginal tokenizer vocab size: 50257\nAdded 7 new tokens.\nNew tokenizer vocab size: 50264\nTokenizer pad token set to: <|pad|>\nTokenizer saved to /kaggle/working/recipe-gpt2-model\n\n--- SETUP COMPLETE ---\nAll libraries and the tokenizer are now correctly loaded.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom datasets import load_dataset, DatasetDict\nprint(\"Loading and tokenizing datasets...\")\n\nnum_proc = os.cpu_count()\nprint(f\"Using {num_proc} processes for tokenization.\")\n\nTRAIN_SUBSET_SIZE = 20000\nVAL_SUBSET_SIZE = 2000\nprint(f\"Loading full dataset to create a {TRAIN_SUBSET_SIZE}-sample subset...\")\n\nfull_dataset = load_dataset('text', data_files={'train': TRAIN_FILE, 'validation': VAL_FILE})\ntrain_subset = full_dataset['train'].shuffle(seed=30).select(range(TRAIN_SUBSET_SIZE))\nval_subset = full_dataset['validation'].shuffle(seed=30).select(range(VAL_SUBSET_SIZE))\n\ndataset = DatasetDict({\n    'train': train_subset,\n    'validation': val_subset\n})\n\nprint(f\"Dataset subset created: {TRAIN_SUBSET_SIZE} train, {VAL_SUBSET_SIZE} validation samples.\")\n\n\nblock_size = 512\ndef tokenize_function(examples):\n\n    return tokenizer(\n        examples['text'],\n        padding='max_length',\n        truncation=True,\n        max_length=block_size,\n    )\n\ntokenized_datasets = dataset.map(\n    tokenize_function,\n    batched=True,\n    num_proc=num_proc,\n    remove_columns=[\"text\"]\n)\n\nprint(\"\\nTokenization complete.\")\nprint(f\"Training dataset features: {tokenized_datasets['train'].features}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T07:32:17.717509Z","iopub.execute_input":"2025-11-02T07:32:17.718170Z","iopub.status.idle":"2025-11-02T07:32:30.976722Z","shell.execute_reply.started":"2025-11-02T07:32:17.718145Z","shell.execute_reply":"2025-11-02T07:32:30.976006Z"}},"outputs":[{"name":"stdout","text":"Loading and tokenizing datasets...\nUsing 4 processes for tokenization.\nLoading full dataset to create a 20000-sample subset...\nDataset subset created: 20000 train, 2000 validation samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8180da5a9b754e13b38bcf4d959d697c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55f4702d2f9d49f6927504d2e58321e8"}},"metadata":{}},{"name":"stdout","text":"\nTokenization complete.\nTraining dataset features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"Initializing model...\")\n\n\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n\nmodel.resize_token_embeddings(len(tokenizer))\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\nprint(f\"Model embedding matrix resized to {len(tokenizer)} to match new tokens.\")\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, \n    mlm=False\n)\n\nprint(\"Model and Data Collator initialized successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T07:32:39.777766Z","iopub.execute_input":"2025-11-02T07:32:39.778049Z","iopub.status.idle":"2025-11-02T07:32:41.023171Z","shell.execute_reply.started":"2025-11-02T07:32:39.778027Z","shell.execute_reply":"2025-11-02T07:32:41.022493Z"}},"outputs":[{"name":"stdout","text":"Initializing model...\nModel embedding matrix resized to 50264 to match new tokens.\nModel and Data Collator initialized successfully.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(\"Defining training arguments\")\n\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=4,       \n    fp16=True,\n    \n  \n    evaluation_strategy=\"steps\",\n    eval_steps=250,\n    logging_steps=100,\n    save_steps=500,\n    \n    save_total_limit=3,\n    load_best_model_at_end=True,\n    report_to=\"none\",\n    dataloader_num_workers=2,\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n)\n\nprint(\"Trainer initialized. Ready for training.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T07:32:47.043802Z","iopub.execute_input":"2025-11-02T07:32:47.044341Z","iopub.status.idle":"2025-11-02T07:32:47.194106Z","shell.execute_reply.started":"2025-11-02T07:32:47.044310Z","shell.execute_reply":"2025-11-02T07:32:47.193398Z"}},"outputs":[{"name":"stdout","text":"Defining training arguments\nTrainer initialized. Ready for training.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"--- Starting Model Training ---\")\ntrainer.train()\nprint(\"--- Training Complete ---\")\ntrainer.save_model()\nprint(f\"Final model and tokenizer saved to {OUTPUT_DIR}\")\nimport math\ntry:\n    eval_results = trainer.evaluate()\n    print(f\"Final Validation Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\nexcept Exception as e:\n    print(f\"Could not calculate final perplexity: {e}\")\n\nprint(f\"\\nFine-Tuning is complete. Your model is saved in {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T07:32:53.863830Z","iopub.execute_input":"2025-11-02T07:32:53.864120Z","iopub.status.idle":"2025-11-02T08:08:53.564638Z","shell.execute_reply.started":"2025-11-02T07:32:53.864100Z","shell.execute_reply":"2025-11-02T08:08:53.563894Z"}},"outputs":[{"name":"stdout","text":"--- Starting Model Training ---\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [625/625 34:34, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>2.459800</td>\n      <td>2.260021</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.291200</td>\n      <td>2.185492</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Checkpoint destination directory /kaggle/working/recipe-gpt2-model/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"--- Training Complete ---\nFinal model and tokenizer saved to /kaggle/working/recipe-gpt2-model\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 01:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Final Validation Perplexity: 8.90\n\nFine-Tuning is complete. Your model is saved in /kaggle/working/recipe-gpt2-model\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\nprint(\"Loading fine-tuned model and tokenizer...\")\n\n\nMODEL_DIR = \"/kaggle/working/recipe-gpt2-model\"\n\n\ntokenizer = GPT2Tokenizer.from_pretrained(MODEL_DIR)\n\nmodel = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval() \n\nprint(f\"Model loaded and moved to {device}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:13:20.319347Z","iopub.execute_input":"2025-11-02T08:13:20.319696Z","iopub.status.idle":"2025-11-02T08:13:20.842824Z","shell.execute_reply.started":"2025-11-02T08:13:20.319671Z","shell.execute_reply":"2025-11-02T08:13:20.842023Z"}},"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"Loading fine-tuned model and tokenizer...\nModel loaded and moved to cuda.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\nBOS_TOKEN = \"<|startofrecipe|>\"\nGENRE_TOKEN = \"<|genre|>\"\nTITLE_TOKEN = \"<|title|>\"\nINGREDIENTS_TOKEN = \"<|ingredients|>\"\nSTEPS_TOKEN = \"<|steps|>\"\nEOS_TOKEN = \"<|endofrecipe|>\"\n\ndef generate_recipe(prompt_title, prompt_genre):\n    print(f\"--- Generating recipe for: {prompt_title} ({prompt_genre}) ---\")\n    prompt_text = (\n        f\"{BOS_TOKEN}\"\n        f\"{GENRE_TOKEN}{prompt_genre}\"\n        f\"{TITLE_TOKEN}{prompt_title}\"\n        f\"{INGREDIENTS_TOKEN}\" \n    )\n    \n    input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n\n    output_sequences = model.generate(\n        input_ids=input_ids,\n        max_length=512,                  \n        temperature=1.0,                 \n        top_k=50,                        \n        top_p=0.95,                      \n        do_sample=True,                  \n        num_return_sequences=1,          \n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n\n    \n    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=False)\n    \n    \n    clean_text = generated_text.replace(prompt_text, \"\").replace(EOS_TOKEN, \"\").strip()\n    \n    \n    clean_text = clean_text.replace(STEPS_TOKEN, f\"\\n\\n{STEPS_TOKEN}\\n\")\n    clean_text = clean_text.replace(INGREDIENTS_TOKEN, f\"{INGREDIENTS_TOKEN}\\n\")\n    \n    print(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:13:28.631463Z","iopub.execute_input":"2025-11-02T08:13:28.632001Z","iopub.status.idle":"2025-11-02T08:13:28.637796Z","shell.execute_reply.started":"2025-11-02T08:13:28.631976Z","shell.execute_reply":"2025-11-02T08:13:28.636930Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Example 1: A dessert\ngenerate_recipe(prompt_title=\"Black Bean And Turkey Chili\", prompt_genre=\"sides\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Example 2: A side dish\ngenerate_recipe(prompt_title=\"Spicy Garlic Potatoes\", prompt_genre=\"sides\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Example 3: A main course\ngenerate_recipe(prompt_title=\"Simple Chicken Curry\", prompt_genre=\"non-veg\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:17:01.503225Z","iopub.execute_input":"2025-11-02T08:17:01.503962Z","iopub.status.idle":"2025-11-02T08:17:07.391736Z","shell.execute_reply.started":"2025-11-02T08:17:01.503939Z","shell.execute_reply":"2025-11-02T08:17:07.390942Z"}},"outputs":[{"name":"stdout","text":"--- Generating recipe for: Black Bean And Turkey Chili (sides) ---\n<|startofrecipe|> <|genre|> sides <|title|> Black Bean And Turkey Chili <|ingredients|>\n bacon, cumin, garlic, ground beef, ground pepper, onion, pepper, water, taco seasoning, zucchini, tomatoes, yellow onion \n\n<|steps|>\n 1. Mix all ingredients together. Cook over low heat, stirring constantly. 2. Sprinkle all over. 3. Serve at room temperature.\n\n==================================================\n\n--- Generating recipe for: Spicy Garlic Potatoes (sides) ---\n<|startofrecipe|> <|genre|> sides <|title|> Spicy Garlic Potatoes <|ingredients|>\n 3, 2, 1/4 cup, 30 seconds, about 3/4, another 1/4 cup, at least 30 minutes, Bake, baking powder, garlic powder, ground cinnamon, ground nutmeg, ground ginger, ground sage, ground thyme, light cornmeal, pecans, olive oil, onion, salt, Season, soy sauce, water \n\n<|steps|>\n 1. Heat oil in Dutch oven. 2. Add the garlic. 3. Brown evenly, about 3/4 full. 4. Add cornmeal and turn over to heat. 5. Add the onion and carrot and cook 5 min. 6. Cook, stirring occasionally, until the onions are tender, about 3/4 full. 7. Add the potatoes. 8. Mix well. 9. Return the garlic mixture to the Dutch oven for 30 seconds, or until the potatoes are browned and the mixture has reached 1/4 full. 10. Cover and let cook until the potatoes are almost cooked through, about 3/4 full. 11. Let set aside. 12. With the potato mixture in a heavy saucepan and the oil in a mixing bowl, add the onions, soy sauce and water. 13. Bring to a boil over low heat and cook, stirring frequently, until the potatoes are tender, about 3/4 full. 14. Add the tomatoes and the garlic. 15. Blend well. 16. Cool on a wire rack for 30 seconds. 17. Transfer potatoes from pan to racks with aluminum foil or aluminum foil and refrigerate, uncovered. 18. For large tortillas, combine the flour, ginger, nutmeg and salt, and blend well. 19. Top with the potatoes. 20. Arrange on a serving platter. 21. Sprinkle with sliced fresh sage and serve.\n\n==================================================\n\n--- Generating recipe for: Simple Chicken Curry (non-veg) ---\n<|startofrecipe|> <|genre|> non-veg <|title|> Simple Chicken Curry <|ingredients|>\n 2, 3, about 15 minutes, Add, egg yolks, garlic, chicken broth, chicken breast, onion, paprika, Put, Stir, vegetable oil, water \n\n<|steps|>\n 1. Wash chicken breast in boiling water. 2. Add all other ingredients. 3. Cook at 350° for 20 minutes or until chicken is tender. 4. Stir occasionally. 5. Transfer mixture to serving platter, tossing with fork if desired.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import evaluate\nimport re \nimport math\ntry:\n    ppl = math.exp(eval_results['eval_loss'])\nexcept NameError:\n    ppl = 8.90 # Fallback\n\nprint(f\"--- Evaluating NEW Model (Perplexity {ppl:.2f}) ---\")\nrouge = evaluate.load('rouge')\nbleu = evaluate.load('bleu')\nGENRE_TOKEN_ESC = re.escape(GENRE_TOKEN)\nTITLE_TOKEN_ESC = re.escape(TITLE_TOKEN)\nINGREDIENTS_TOKEN_ESC = re.escape(INGREDIENTS_TOKEN)\n\n\ntry:\n    with open(\"/kaggle/working/val_recipes.txt\", \"r\", encoding=\"utf-8\") as f:\n        reference_recipe_full = f.readline()\n    \n    ref_genre = re.search(f'{GENRE_TOKEN_ESC}(.*?){TITLE_TOKEN_ESC}', reference_recipe_full).group(1)\n    ref_title = re.search(f'{TITLE_TOKEN_ESC}(.*?){INGREDIENTS_TOKEN_ESC}', reference_recipe_full).group(1)\n    \n    reference_text = INGREDIENTS_TOKEN + reference_recipe_full.split(INGREDIENTS_TOKEN)[1]\n    reference_text = reference_text.replace(EOS_TOKEN, \"\").strip()\n\n    print(f\"--- Evaluating with reference recipe: {ref_title} ({ref_genre}) ---\")\n\n   \n    prompt_text = (\n        f\"{BOS_TOKEN}\"\n        f\"{GENRE_TOKEN}{ref_genre}\"\n        f\"{TITLE_TOKEN}{ref_title}\"\n        f\"{INGREDIENTS_TOKEN}\"\n    )\n    input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n    \n    output_sequences = model.generate(\n        input_ids=input_ids,\n        max_length=512,\n        do_sample=False, \n        num_beams=3,     \n        pad_token_id=tokenizer.pad_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n        repetition_penalty=1.2,\n        no_repeat_ngram_size=2  \n    )\n    \n\n    prediction_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n    clean_prompt = f\"{ref_genre}{ref_title}\"\n    prediction_text = prediction_text.replace(clean_prompt, \"\").strip()\n\n    \n    reference_text_cleaned = reference_text.replace(INGREDIENTS_TOKEN, \"\").replace(STEPS_TOKEN, \" \").strip()\n\n    print(\"\\n--- ROUGE Scores (Recall-Oriented) ---\")\n    rouge_results = rouge.compute(predictions=[prediction_text], references=[reference_text_cleaned])\n    print(f\"ROUGE-1 (Unigram): {rouge_results['rouge1']:.4f}\")\n    print(f\"ROUGE-2 (Bigram): {rouge_results['rouge2']:.4f}\")\n    print(f\"ROUGE-L (Longest Common Subsequence): {rouge_results['rougeL']:.4f}\")\n    \n    print(\"\\n--- BLEU Score (Precision-Oriented) ---\")\n    bleu_results = bleu.compute(predictions=[prediction_text], references=[[reference_text_cleaned]])\n    print(f\"BLEU: {bleu_results['bleu']:.4f}\")\n    \n    print(\"\\n--- COMPARISON ---\")\n    print(\"\\n**REFERENCE (Ground Truth):**\\n\", reference_text_cleaned[:500] + \"...\")\n    print(\"\\n**PREDICTION (Generated):**\\n\", prediction_text[:500] + \"...\")\n\nexcept Exception as e:\n    print(f\"Could not run evaluation: {e}\")\n    print(\"Please ensure 'val_recipes.txt' exists in /kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T08:24:12.529594Z","iopub.execute_input":"2025-11-02T08:24:12.530316Z","iopub.status.idle":"2025-11-02T08:24:15.883709Z","shell.execute_reply.started":"2025-11-02T08:24:12.530294Z","shell.execute_reply":"2025-11-02T08:24:15.883071Z"}},"outputs":[{"name":"stdout","text":"--- Evaluating NEW Model (Perplexity 8.90) ---\n--- Evaluating with reference recipe: The Best Tres Leches Cake (bakery) ---\n\n--- ROUGE Scores (Recall-Oriented) ---\nROUGE-1 (Unigram): 0.3262\nROUGE-2 (Bigram): 0.0693\nROUGE-L (Longest Common Subsequence): 0.1803\n\n--- BLEU Score (Precision-Oriented) ---\nBLEU: 0.0000\n\n--- COMPARISON ---\n\n**REFERENCE (Ground Truth):**\n 15-20 minute, 25-35 minute, 350 degrees, cold.(at least 1-2 hours, condensed milk, eggs, milk, overnight, three, toothpick, topping, water, yellow cake 1. Mix together the cake mix, eggs, and water in a large bowl. 2. Pour into a greased Large baking pan, and bake at 350 degrees for 25-35 minute or until toothpick comes out clean. 3. Meanwhile, in a large sauce pan heat up the three milks, and whisk until well combined. 4. Take the cake out of the oven, and let it cool for 15-20 minute Once cool...\n\n**PREDICTION (Generated):**\n 1/2, 1/4 cup, 30 minutes, 350\\u00b0, Bake, baking powder, butter, eggs, flour, milk, salt, sugar, vanilla1. Preheat oven to 350°F. 2. In a large bowl, beat eggs until light and fluffy. 3. Add flour and salt; mix well. 4. Pour into greased 9 x 13-in. baking pan. 5. Bake in preheated oven for 1 hour or until golden brown. 6. Remove from oven and cool completely. 7. Cool on wire rack....\n","output_type":"stream"}],"execution_count":20}]}